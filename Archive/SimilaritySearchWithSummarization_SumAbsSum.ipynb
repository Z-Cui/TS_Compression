{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt # python library for wavelet transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An interface to calculate euclidean distance between 2 signals\n",
    "def euc_dist(x,y):\n",
    "    return float(np.linalg.norm(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "filename_synthetic = 'synthetic_size50k_len256_znorm.bin'\n",
    "with open(filename_synthetic, 'rb') as in_file:\n",
    "    time_series_synthetic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)\n",
    "\n",
    "filename_seismic = 'seismic_size50k_len256_znorm.bin'\n",
    "with open(filename_seismic, 'rb') as in_file:\n",
    "    time_series_seismic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)\n",
    "    \n",
    "filename_seismic_query = 'seismic-query_size100_len256_znorm.bin'\n",
    "with open(filename_seismic_query, 'rb') as in_file:\n",
    "    time_series_seismic_query = np.array(struct.unpack('f' * 100 * 256, in_file.read())).reshape(-1,256)\n",
    "\n",
    "filename_synthetic_query = 'synthetic-query_size100_len256_znorm.bin'\n",
    "with open(filename_synthetic_query, 'rb') as in_file:\n",
    "    time_series_synthetic_query = np.array(struct.unpack('f' * 100 * 256, in_file.read())).reshape(-1,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_to_size(x, size):\n",
    "    size = int(size/4 /2)\n",
    "    piece_size = int(256/size)\n",
    "    com_x = np.array([], dtype = np.float32).reshape(x.shape[0], -1)\n",
    "    for i in range(size):\n",
    "        sum_segment = np.sum(x[:, int(piece_size*i):int(piece_size*i+piece_size-1)], axis = 1)\n",
    "        abssum_segment = np.sum(np.abs(x[:, int(piece_size*i):int(piece_size*i+piece_size-1)]), axis = 1)\n",
    "        com_x = np.column_stack((com_x, sum_segment))\n",
    "        com_x = np.column_stack((com_x, abssum_segment))\n",
    "    return com_x\n",
    "def decompress1024(x, size):\n",
    "    size = int(size/4 /2)\n",
    "    piece_size = int(256/size)\n",
    "    rec_x = np.array([], dtype = np.float32).reshape(x.shape[0], -1)\n",
    "    for i in range(size):\n",
    "        deviationSegment = x[:, 2*i] / piece_size\n",
    "        deviationPoint = x[:, 2*i+1] / piece_size\n",
    "        for j in range(int(piece_size/2)):\n",
    "            m1 = deviationSegment + deviationPoint\n",
    "            m2 = deviationSegment - deviationPoint\n",
    "            rec_x = np.column_stack((rec_x, m1))\n",
    "            rec_x = np.column_stack((rec_x, m2))\n",
    "    return rec_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_euclidean_compression(q, x_com, x_rec, compress_size):\n",
    "    \n",
    "    # Compress the query signal to a smaller time series which contains \"compress_size\" float numbers\n",
    "    q_com = compress_to_size(q, compress_size)\n",
    "    #x_com = compress_to_size(x, compress_size)\n",
    "    #x_rec = decompress1024(x_com, compress_size)\n",
    "\n",
    "    D_closest_com = 10000.0  # Initialize closest distance for compressed time series as infinitiy\n",
    "    D_closest_ori = 10000.0  # Initialize closest distance for original time series as infinitiy\n",
    "    index_closest = 0        # Initialize closest time series' index as 0\n",
    "    pruning_count = 0        # Initialize pruning count as 0\n",
    "    \n",
    "    for i in range(50000): # Loop for all time_series\n",
    "        \n",
    "        # Calculate euclidean distance between query and compressed time series\n",
    "        dist_com = euc_dist(q_com, x_com[i,]) \n",
    "        \n",
    "        if D_closest_com > dist_com:\n",
    "            \n",
    "            # Calculate the euclidean distance between query and reconstructed time series\n",
    "            dist_ori = euc_dist(q, x_rec[i,]) \n",
    "            \n",
    "            if D_closest_ori > dist_ori:\n",
    "                \n",
    "                # Memorization\n",
    "                D_closest_com = dist_com\n",
    "                D_closest_ori = dist_ori\n",
    "                index_closest = i\n",
    "        \n",
    "        # When euclidean distance between compressed time series is bigger\n",
    "        else:\n",
    "            # The time series is pruned, we count all pruned times\n",
    "            pruning_count += 1\n",
    "            \n",
    "    # Return index, distance and pruning times for this similarity search\n",
    "    return D_closest_ori, pruning_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_euclidean_no_compression(q, time_series):\n",
    "\n",
    "    D_closest_ori = 10000.0  # Initialize original minimum distance as infinitiy\n",
    "    \n",
    "    for i in range(time_series.shape[0]): # Loop for all time_series\n",
    "        x = time_series[i,]\n",
    "        dist_ori = euc_dist(q, x) # Calculate the euclidean distance between original time series\n",
    "        \n",
    "        if D_closest_ori > dist_ori:\n",
    "            D_closest_ori = dist_ori\n",
    "            \n",
    "    # Return index, distance for this similarity search\n",
    "    return D_closest_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d168f6822caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_euclidean_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdistSimilarity_128_synthetic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpruning_count_128_synthetic\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpruning_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-478789bbf026>\u001b[0m in \u001b[0;36msimilarity_euclidean_compression\u001b[0;34m(q, x_com, x_rec, compress_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Compress the query signal to a smaller time series which contains \"compress_size\" float numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mq_com\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_to_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#x_com = compress_to_size(x, compress_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#x_rec = decompress1024(x_com, compress_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0b179fea5721>\u001b[0m in \u001b[0;36mcompress_to_size\u001b[0;34m(x, size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcom_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msum_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mabssum_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpiece_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcom_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "pruning_count_128_synthetic = 0\n",
    "pruning_count_64_synthetic = 0\n",
    "pruning_count_32_synthetic = 0\n",
    "\n",
    "distSimilarity_128_synthetic = []\n",
    "distSimilarity_64_synthetic = []\n",
    "distSimilarity_32_synthetic = []\n",
    "distSimilarity_synthetic = []\n",
    "\n",
    "x = time_series_synthetic\n",
    "\n",
    "sum_128 = compress_to_size(x, 128)\n",
    "sum_64 = compress_to_size(x, 64)\n",
    "sum_32 = compress_to_size(x, 32)\n",
    "\n",
    "rec_128 = decompress1024(sum_128, 128)\n",
    "rec_64 = decompress1024(sum_64, 64)\n",
    "rec_32 = decompress1024(sum_32, 32)\n",
    "\n",
    "query_size = 100\n",
    "for i in range(query_size):\n",
    "    \n",
    "    # Synthetic\n",
    "    q = time_series_synthetic_query[i,]\n",
    "    \n",
    "    # 128\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_128, rec_128, 128)\n",
    "    distSimilarity_128_synthetic.append(dist)\n",
    "    pruning_count_128_synthetic += pruning_count\n",
    "    \n",
    "    # 64\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_64, rec_64, 64)\n",
    "    distSimilarity_64_synthetic.append(dist)\n",
    "    pruning_count_64_synthetic += pruning_count\n",
    "    \n",
    "    # 32\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_32, rec_32, 32)\n",
    "    distSimilarity_32_synthetic.append(dist)\n",
    "    pruning_count_32_synthetic += pruning_count\n",
    "    \n",
    "    # without data summarization\n",
    "    dist_no_com = similarity_euclidean_no_compression(q, time_series_synthetic)\n",
    "    distSimilarity_synthetic.append(dist_no_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_count_128_seismic = 0\n",
    "pruning_count_64_seismic = 0\n",
    "pruning_count_32_seismic = 0\n",
    "\n",
    "distSimilarity_128_seismic = []\n",
    "distSimilarity_64_seismic = []\n",
    "distSimilarity_32_seismic = []\n",
    "distSimilarity_seismic = []\n",
    "\n",
    "x = time_series_seismic\n",
    "\n",
    "sum_128 = compress_to_size(x, 128)\n",
    "sum_64 = compress_to_size(x, 64)\n",
    "sum_32 = compress_to_size(x, 32)\n",
    "\n",
    "rec_128 = decompress1024(sum_128, 128)\n",
    "rec_64 = decompress1024(sum_64, 64)\n",
    "rec_32 = decompress1024(sum_32, 32)\n",
    "\n",
    "query_size = 100\n",
    "for i in range(query_size):\n",
    "    # Seismic\n",
    "    q = time_series_seismic_query[i,]\n",
    "    \n",
    "    # 128\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_128, rec_128, 128)\n",
    "    distSimilarity_128_seismic.append(dist)\n",
    "    pruning_count_128_seismic += pruning_count\n",
    "    \n",
    "    # 64\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_64, rec_64, 64)\n",
    "    distSimilarity_64_seismic.append(dist)\n",
    "    pruning_count_64_seismic += pruning_count\n",
    "    \n",
    "    # 32\n",
    "    dist, pruning_count = similarity_euclidean_compression(q, sum_32, rec_32, 32)\n",
    "    distSimilarity_32_seismic.append(dist)\n",
    "    pruning_count_32_seismic += pruning_count\n",
    "    \n",
    "    # without data summarization\n",
    "    dist_no_com = similarity_euclidean_no_compression(q, time_series_seismic)\n",
    "    distSimilarity_seismic.append(dist_no_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "\n",
    "print(\"Similarity search for synthetic time series:\")\n",
    "print(\"Average pruning ratio 128/64/32: \", pruning_count_128_synthetic/size/50000, \n",
    "      pruning_count_64_synthetic/size/50000, pruning_count_32_synthetic/size/50000)\n",
    "\n",
    "print(\"Similarity search for seismic time series:\")\n",
    "print(\"Average pruning ratio 128/64/32: \", pruning_count_128_seismic/size/50000, \n",
    "      pruning_count_64_seismic/size/50000, pruning_count_32_seismic/size/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_data(x):\n",
    "    print(\"Average:\", np.mean(x).round(4), \" Mean:\", np.median(x).round(4), \" Max:\", np.max(x).round(4), \" Min:\", np.min(x).round(4), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Synthetic time series: \")\n",
    "print(\"---------------------\")\n",
    "print(\"Similarity search error - with data summarization (128)\")\n",
    "summarize_data(distSimilarity_128_synthetic)\n",
    "print(\"Similarity search error - with data summarization (64)\")\n",
    "summarize_data(distSimilarity_64_synthetic)\n",
    "print(\"Similarity search error - with data summarization (32)\")\n",
    "summarize_data(distSimilarity_32_synthetic)\n",
    "print(\"Similarity search error - without data summarization\")\n",
    "summarize_data(distSimilarity_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seismic time series: \")\n",
    "print(\"---------------------\")\n",
    "print(\"Similarity search error - with data summarization (128)\")\n",
    "summarize_data(distSimilarity_128_seismic)\n",
    "print(\"Similarity search error - with data summarization (64)\")\n",
    "summarize_data(distSimilarity_64_seismic)\n",
    "print(\"Similarity search error - with data summarization (32)\")\n",
    "summarize_data(distSimilarity_32_seismic)\n",
    "print(\"Similarity search error - without data summarization\")\n",
    "summarize_data(distSimilarity_seismic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.boxplot([distSimilarity_128_seismic, distSimilarity_64_seismic, distSimilarity_32_seismic, distSimilarity_seismic])\n",
    "plt.title('Similarity search error - 100 seismic time series')\n",
    "plt.xticks([1, 2, 3, 4], ['128', '64', '32', 'no_compression'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot([distSimilarity_128_synthetic, distSimilarity_64_synthetic, distSimilarity_32_synthetic, distSimilarity_synthetic])\n",
    "plt.title('Similarity search error - 100 synthetic time series')\n",
    "plt.xticks([1, 2, 3, 4], ['128', '64', '32', 'no_compression'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
