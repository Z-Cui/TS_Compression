{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We used Discrete Wavelet Transformation (DWT) to complete this project\n",
    "# Task 1, Test 1 for Data Summarization; \n",
    "# Task 2, Test 2 for Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Time Series Data Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt # python library for wavelet transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Serie Compression and Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress of a time_serie x by using Daubechies Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression of data set 'x' to half size of 'x', e.g. 256 -> 128\n",
    "def compress(x):\n",
    "    compressed_x, cD = pywt.dwt(x, 'db1')\n",
    "    return compressed_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction of a time_serie from y by using Daubechies Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction of data set 'y' to twice size of 'y', e.g. 128 -> 256\n",
    "def reconstruct(y):\n",
    "    reconstructed_y = pywt.idwt(y, None, 'db1')\n",
    "    return reconstructed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Compression of a time_serie x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress 'x' i times and get a [x.size / 2**i] size data\n",
    "# e.g. using com_iterative(x, 3) on x which contains 256 float numbers,\n",
    "#      we'll have a compressed [256 / 2**3] = 32 float numbers\n",
    "def com_iterative(x, i):\n",
    "    for k in range(i):\n",
    "        compressed_x = compress(x)\n",
    "        x = compressed_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Decompression from a time_serie y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction of data set 'y' to [y.original_size * 2**i]\n",
    "def rec_iterative(y, i):\n",
    "    tmp = y\n",
    "    for k in range(i):\n",
    "        reconstructed_y = reconstruct(tmp)\n",
    "        tmp = reconstructed_y\n",
    "    return reconstructed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfaces to facilitate using compression/reconstruction functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface functions to encode/decode easily\n",
    "def compress_query(x, size):\n",
    "    if size == 16 or size == 32 or size == 64 or size == 128:\n",
    "        return com_iterative(x, int(math.log(x.shape[0]/size, 2)))\n",
    "    else:\n",
    "        print(\"compress_query(x,size): size is not valid.\")\n",
    "        import os\n",
    "        os.system('pause')\n",
    "\n",
    "def decoder256(y):\n",
    "    return rec_iterative(y, int(math.log(256/y.shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Compression and Reconstruction with given 50k time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 50k time_series (seismic and synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "# load data set: 50000 seismic signals\n",
    "filename_seismic = 'seismic_size50k_len256_znorm.bin'\n",
    "with open(filename_seismic, 'rb') as in_file:\n",
    "    time_series_seismic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)\n",
    "\n",
    "# load data set: 50000 synthetic signals\n",
    "filename_synthetic = 'synthetic_size50k_len256_znorm.bin'\n",
    "with open(filename_synthetic, 'rb') as in_file:\n",
    "    time_series_synthetic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data compression and reconstruction for 50k seismic time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall reconstruction error for  50000  seismic time series: (128 / 64 / 32 / 16)\n",
      "632932.170386034 771074.4015698319 790471.3970702328 797346.2607445099\n",
      "Average reconstruction error for  50000  seismic time series: (128 / 64 / 32 / 16)\n",
      "12.658643407720678 15.421488031396638 15.809427941404655 15.946925214890197\n"
     ]
    }
   ],
   "source": [
    "# Following code is to compress and reconstruct seismic data\n",
    "# every time serie will be compressed from 256 to 128/64/32/16 \n",
    "# then the compressed data will be \"enlarged\" from 128/64/32/16 to 256\n",
    "\n",
    "# The variables below is used to SAVE time serie compression and reconstruction for seismic data:\n",
    "'''\n",
    "resized128_seismic = np.empty((0,128),float)\n",
    "resized64_seismic = np.empty((0,64),float)\n",
    "resized32_seismic = np.empty((0,32),float)\n",
    "resized16_seismic = np.empty((0,16),float)\n",
    "\n",
    "reconstructed_128to256_seismic = np.empty((0,256),float)\n",
    "reconstructed_64to256_seismic = np.empty((0,256),float)\n",
    "reconstructed_32to256_seismic = np.empty((0,256),float)\n",
    "reconstructed_16to256_seismic = np.empty((0,256),float)\n",
    "'''\n",
    "\n",
    "# Initialize variables to record reconstruction error for 50k time_series\n",
    "error_128_seismic = 0.0\n",
    "error_64_seismic = 0.0\n",
    "error_32_seismic = 0.0\n",
    "error_16_seismic = 0.0\n",
    "\n",
    "data_size = 50000\n",
    "for i in range(data_size):\n",
    "    \n",
    "    x = time_series_seismic[i,] # x contains 256 float numbers\n",
    "    \n",
    "    # 256 -> 128\n",
    "    resized_128 = compress_query(x, 128)\n",
    "    reconstructed_128 = decoder256(resized_128)\n",
    "    error_128_seismic = error_128_seismic + float(np.linalg.norm(x - reconstructed_128))\n",
    "    \n",
    "    # 256 -> 64\n",
    "    resized_64 = compress_query(x, 64)\n",
    "    reconstructed_64 = decoder256(resized_64)\n",
    "    error_64_seismic = error_64_seismic + float(np.linalg.norm(x - reconstructed_64))\n",
    "    \n",
    "    # 256 -> 32\n",
    "    resized_32 = compress_query(x, 32)\n",
    "    reconstructed_32 = decoder256(resized_32)\n",
    "    error_32_seismic = error_32_seismic + float(np.linalg.norm(x - reconstructed_32))\n",
    "    \n",
    "    # 256 -> 16\n",
    "    resized_16 = compress_query(x, 16)\n",
    "    reconstructed_16 = decoder256(resized_16)\n",
    "    error_16_seismic = error_16_seismic + float(np.linalg.norm(x - reconstructed_16))\n",
    "    \n",
    "    # Following code is to save compression and reconstruction to an array,\n",
    "    # saving data takes much time than you could imagine\n",
    "    # Please use the saved data in the folder \"compressed_50k\", \n",
    "    # an simple instruction to load data in the \"ReadMe\" file.\n",
    "    '''\n",
    "    resized128_seismic = np.vstack((resized128_seismic, resized_128))\n",
    "    reconstructed_128to256_seismic = np.vstack((reconstructed_128to256_seismic, reconstructed_128))\n",
    "    resized64_seismic = np.vstack((resized64_seismic, resized_64))\n",
    "    reconstructed_64to256_seismic = np.vstack((reconstructed_64to256_seismic, reconstructed_64))\n",
    "    resized32_seismic = np.vstack((resized32_seismic, resized_32))\n",
    "    reconstructed_32to256_seismic = np.vstack((reconstructed_32to256_seismic,reconstructed_32))\n",
    "    resized16_seismic = np.vstack((resized16_seismic, resized_16))\n",
    "    reconstructed_16to256_seismic = np.vstack((reconstructed_16to256_seismic,reconstructed_16))\n",
    "\n",
    "np.save(\"compressed128_50k_seismic.npy\", resized128_seismic)\n",
    "np.save(\"compressed64_50k_seismic.npy\", resized64_seismic)\n",
    "np.save(\"compressed32_50k_seismic.npy\", resized32_seismic)\n",
    "np.save(\"compressed16_50k_seismic.npy\", resized16_seismic)\n",
    "\n",
    "np.save(\"reconstructed_128to256_50k_seismic.npy\", reconstructed_128to256_seismic)\n",
    "np.save(\"reconstructed_64to256_50k_seismic.npy\", reconstructed_64to256_seismic)\n",
    "np.save(\"reconstructed_32to256_50k_seismic.npy\", reconstructed_32to256_seismic)\n",
    "np.save(\"reconstructed_16to256_50k_seismic.npy\", reconstructed_16to256_seismic)\n",
    "'''\n",
    "\n",
    "# Print the reconstruction error for 50k time_series\n",
    "print(\"Overall reconstruction error for \", data_size, \" seismic time series: (128 / 64 / 32 / 16)\")\n",
    "print(error_128_seismic, error_64_seismic, error_32_seismic, error_16_seismic)\n",
    "print(\"Average reconstruction error for \", data_size, \" seismic time series: (128 / 64 / 32 / 16)\")\n",
    "print(error_128_seismic/data_size, error_64_seismic/data_size, error_32_seismic/data_size, error_16_seismic/data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data compression and reconstruction for 50k synthetic time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall reconstruction error for  50000  synthetic time series: (128 / 64 / 32 / 16)\n",
      "76273.23288734269 120327.73912184678 173555.64466821024 244335.4583699993\n",
      "Average reconstruction error for  50000  synthetic time series: (128 / 64 / 32 / 16)\n",
      "1.5254646577468538 2.4065547824369355 3.4711128933642046 4.886709167399986\n"
     ]
    }
   ],
   "source": [
    "# Following code is to compress and reconstruct seismic data\n",
    "# every time serie will be compressed from 256 to 128/64/32/16 \n",
    "# then the compressed data will be \"enlarged\" from 128/64/32/16 to 256\n",
    "\n",
    "\n",
    "# The variables below is used to SAVE time serie compression and reconstruction for 50k synthetic time_series:\n",
    "'''\n",
    "resized128_synthetic = np.empty((0,128),float)\n",
    "resized64_synthetic = np.empty((0,64),float)\n",
    "resized32_synthetic = np.empty((0,32),float)\n",
    "resized16_synthetic = np.empty((0,16),float)\n",
    "\n",
    "reconstructed_128to256_synthetic = np.empty((0,256),float)\n",
    "reconstructed_64to256_synthetic = np.empty((0,256),float)\n",
    "reconstructed_32to256_synthetic = np.empty((0,256),float)\n",
    "reconstructed_16to256_synthetic = np.empty((0,256),float)\n",
    "'''\n",
    "\n",
    "# Initialize variables to record reconstruction error for 50k time_series\n",
    "error_128_synthetic = 0.0\n",
    "error_64_synthetic = 0.0\n",
    "error_32_synthetic = 0.0\n",
    "error_16_synthetic = 0.0\n",
    "\n",
    "data_size = 50000\n",
    "for i in range(data_size):\n",
    "    \n",
    "    x = time_series_synthetic[i,] # x contains 256 float numbers\n",
    "    \n",
    "    # 256 -> 128\n",
    "    resized_128 = compress_query(x, 128)\n",
    "    reconstructed_128 = decoder256(resized_128)\n",
    "    error_128_synthetic = error_128_synthetic + float(np.linalg.norm(x - reconstructed_128))\n",
    "    \n",
    "    # 256 -> 64\n",
    "    resized_64 = compress_query(x, 64)\n",
    "    reconstructed_64 = decoder256(resized_64)\n",
    "    error_64_synthetic = error_64_synthetic + float(np.linalg.norm(x - reconstructed_64))\n",
    "    \n",
    "    # 256 -> 32\n",
    "    resized_32 = compress_query(x, 32)\n",
    "    reconstructed_32 = decoder256(resized_32)\n",
    "    error_32_synthetic = error_32_synthetic + float(np.linalg.norm(x - reconstructed_32))\n",
    "    \n",
    "    # 256 -> 16\n",
    "    resized_16 = compress_query(x, 16)\n",
    "    reconstructed_16 = decoder256(resized_16)\n",
    "    error_16_synthetic = error_16_synthetic + float(np.linalg.norm(x - reconstructed_16))\n",
    "    \n",
    "    # Following code can save compression and reconstruction to an array, saving data takes much time than you could imagine.\n",
    "    # Please use the saved data in the folder \"compressed_50k\", an simple instruction to load data in the \"ReadMe\" file.\n",
    "    '''\n",
    "    resized128_synthetic = np.vstack((resized128_synthetic, resized_128))\n",
    "    reconstructed_128to256_synthetic = np.vstack((reconstructed_128to256_synthetic, reconstructed_128))\n",
    "    resized64_synthetic = np.vstack((resized64_synthetic, resized_64))\n",
    "    reconstructed_64to256_synthetic = np.vstack((reconstructed_64to256_synthetic, reconstructed_64))\n",
    "    resized32_synthetic = np.vstack((resized32_synthetic, resized_32))\n",
    "    reconstructed_32to256_synthetic = np.vstack((reconstructed_32to256_synthetic,reconstructed_32))\n",
    "    resized16_synthetic = np.vstack((resized16_synthetic, resized_16))\n",
    "    reconstructed_16to256_synthetic = np.vstack((reconstructed_16to256_synthetic,reconstructed_16))\n",
    "\n",
    "np.save(\"compressed128_50k_synthetic.npy\", resized128_synthetic)\n",
    "np.save(\"compressed64_50k_synthetic.npy\", resized64_synthetic)\n",
    "np.save(\"compressed32_50k_synthetic.npy\", resized32_synthetic)\n",
    "np.save(\"compressed16_50k_synthetic.npy\", resized16_synthetic)\n",
    "\n",
    "np.save(\"reconstructed_128to256_50k_synthetic.npy\", reconstructed_128to256_synthetic)\n",
    "np.save(\"reconstructed_64to256_50k_synthetic.npy\", reconstructed_64to256_synthetic)\n",
    "np.save(\"reconstructed_32to256_50k_synthetic.npy\", reconstructed_32to256_synthetic)\n",
    "np.save(\"reconstructed_16to256_50k_synthetic.npy\", reconstructed_16to256_synthetic)\n",
    "'''\n",
    "\n",
    "# Print the reconstruction error for 50k time_series\n",
    "print(\"Overall reconstruction error for \", data_size, \" synthetic time series: (128 / 64 / 32 / 16)\")\n",
    "print(error_128_synthetic, error_64_synthetic, error_32_synthetic, error_16_synthetic)\n",
    "print(\"Average reconstruction error for \", data_size, \" synthetic time series: (128 / 64 / 32 / 16)\")\n",
    "print(error_128_synthetic/data_size, error_64_synthetic/data_size, error_32_synthetic/data_size, error_16_synthetic/data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Similarity Search by using Euclidean Distance (with Data Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search by using Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_euclidean(query, time_series, compress_size):\n",
    "    \n",
    "    # Compress the query signal to a smaller time serie which contains \"compress_size\" float numbers\n",
    "    compressed_query = compress_query(query, compress_size)\n",
    "    \n",
    "    # Initialize compressed minimum distance, original minimum distance as infinitiy (here we set as 10000.0)\n",
    "    D_closest_com = 10000.0\n",
    "    D_closest_orginal = 10000.0\n",
    "    # Initialize closest time serie's index as 0\n",
    "    index_closest = 0\n",
    "    # Initialize pruning count as 0\n",
    "    pruning_count = 0\n",
    "    \n",
    "    # Loop for all time_series\n",
    "    for i in range(time_series.shape[0]):\n",
    "        \n",
    "        # Compress i-th time serie as x\n",
    "        x = compress_query(time_series[i,], compress_size)\n",
    "        # Calculate euclidean distance between compressed time series\n",
    "        dist_com = float(np.linalg.norm(x - compressed_query))\n",
    "        \n",
    "        # If the euclidean distance between compressed time series is smaller\n",
    "        if D_closest_com > dist_com:\n",
    "            \n",
    "            # Calculate the euclidean distance between original time series\n",
    "            dist_real = float(np.linalg.norm(query - time_series[i,]))\n",
    "            \n",
    "            # If the euclidean distance between original time series is smaller\n",
    "            if D_closest_orginal > dist_real:\n",
    "                \n",
    "                # Memorization\n",
    "                D_closest_com = dist_com\n",
    "                D_closest_orginal = dist_real\n",
    "                index_closest = i\n",
    "                \n",
    "        # When euclidean distance between compressed time series is bigger\n",
    "        else:\n",
    "            # The time serie is pruned, we count all pruned times\n",
    "            pruning_count += 1\n",
    "            \n",
    "    # Return index, distance and pruning times for this similarity search\n",
    "    return index_closest, D_closest_orginal, pruning_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Similarity Search with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 100 query time_series (seismic and synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "# load data set: 100 seismic signals\n",
    "filename_seismic = 'seismic-query_size100_len256_znorm.bin'\n",
    "with open(filename_seismic, 'rb') as in_file:\n",
    "    time_series_seismic_query = np.array(struct.unpack('f' * 100 * 256, in_file.read())).reshape(-1,256)\n",
    "\n",
    "# load data set: 100 synthetic signals\n",
    "filename_synthetic = 'synthetic-query_size100_len256_znorm.bin'\n",
    "with open(filename_synthetic, 'rb') as in_file:\n",
    "    time_series_synthetic_query = np.array(struct.unpack('f' * 100 * 256, in_file.read())).reshape(-1,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 50k time_series for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "# load data set: 50000 seismic signals\n",
    "filename_seismic = 'seismic_size50k_len256_znorm.bin'\n",
    "with open(filename_seismic, 'rb') as in_file:\n",
    "    time_series_seismic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)\n",
    "\n",
    "# load data set: 50000 synthetic signals\n",
    "filename_synthetic = 'synthetic_size50k_len256_znorm.bin'\n",
    "with open(filename_synthetic, 'rb') as in_file:\n",
    "    time_series_synthetic = np.array(struct.unpack('f' * 50000 * 256, in_file.read())).reshape(-1,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search of a time_serie by using the defined function \"similarity_euclidean(query, time_series, compress_size)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a closest time_serie q with 50k seismic time_series, with compression ratio 16 (256/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest time_serie to the query is  12503 -th time_serie.\n",
      "The closest distance is  0.0\n",
      "For this similarity search,  49567  time_series have been pruned.\n"
     ]
    }
   ],
   "source": [
    "ind_query = 3\n",
    "q = time_series_seismic_query[ind_query,]\n",
    "\n",
    "idx, dist, pruning_count = similarity_euclidean(q, time_series_seismic, 16)\n",
    "\n",
    "print(\"The closest time_serie to the query is \", idx, \"-th time_serie.\") \n",
    "print(\"The closest distance is \", dist)\n",
    "print(\"For this similarity search, \", pruning_count, \" time_series have been pruned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
